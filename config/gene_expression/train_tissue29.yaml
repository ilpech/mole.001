# use false to disable setting
data:
  cashPath: ./bio_dl/cash/dataloader
  datasets2use:
    tissue29: True
    # tissue13: True
    # nci60: True
    # tissue29: False
    tissue13: False
    nci60: False
  tissue29_rna: ./bio_dl/testdata/tissue29_rna.tsv
  tissue29_prot: ./bio_dl/testdata/tissue29_prot.tsv
  tissue13_rna: ./bio_dl/testdata/tissue13_rna.tsv
  tissue13_prot: ./bio_dl/testdata/tissue13_prot.tsv
  nci60_rna: ./bio_dl/testdata/nci60_rna.tsv
  nci60_prot: ./bio_dl/testdata/nci60_prot.tsv
  geneMapping: ./bio_dl/testdata/human_ids_mapping.tab
  engs2uniprot_file: ./bio_dl/testdata/mapping_out/engs2uniprot_human.txt
  skipMissing: False
  splitGeneFromModelConfig: ./bio_dl/testdata/prot_abundance_regressor.full_tissue29_.2_val_config_NONORM.json
  # splitGeneFromModelConfig: False
  # ifSplitPercentToTrain: 0.8 
  ifSplitPercentToTrain: False
  useZeroProt: False
  useNonZeroProt: True
train:
  params_dir: ./trained
  max_var_layer: 2048
  gpus2use: 2
  each_train_has_own_dataset: False
  net_name: rna2protein_tissue29.ResNet34V2.011
  finetune_net_name: rna2protein_tissue29.ResNet34V2.010
  finetune_params_dir: /home/ilpech/repositories/machine_learning/trained
  use_finetune_experiments: True
  epoch_start_from: 5
  epochs: 911
  log_interval: 3
  # metric_flush_interval: 500
  metric_flush_interval: 1
  wd: 0.0001
  momentum: 0.9
  optimizer: sgd
  lr:
    mode: [
      # byHand,
      # byPlateu,
      cosineWarmup
    ]
    metric2use: [
      norm_p2_val,
      # denorm_p2_val,
    ]
    byHand:
      0: 0.003
      1: 0.001
      3: 0.0003
      5: 0.0001
      7: 0.00003
    byPlateu:
      mode: max
      start_value: 0.003
      factor: 0.3
      # number of epochs with no improvement 
      # after which learning rate will be reduced
      patience: 2
      threshold: 0.05
      # number of epochs to wait before 
      # resuming normal operation after lr has been reduced
      cooldown: 2
      min_lr: 0.000001 
    # cosineWarmup:
    #   max_lr: 0.0003
    #   min_lr: 0.0000001
    #   warmup_steps: 199
    #   first_cycle_steps: 359
    #   step_each_batch: 3
    #   cycle_mult: 0.95
    #   # decrease max learning rate by cycle
    #   gamma: 0.85
    cosineWarmup:
      max_lr: 0.0001
      min_lr: 0.0000001
      warmup_steps: 127
      first_cycle_steps: 701
      step_each_batch: 2
      cycle_mult: 0.83
      # decrease max learning rate by cycle
      gamma: 0.97

  loss: [
    # Huber,
    MSE,
    # SmoothL1
  ]
  # batch_size:
  #   0: 150
  batch_size:
    0: 150
  augm:
    isEnabled: True
    epoch_p:
      0: 0.99
      10: 0.8
model:
    model2use: [
      # wideResNet,
      ResNetV2
    ]
    wideResNet:
      num_layers: 34
      width_factor: 3
    ResNetV2:
      num_layers: 34
      width_factor: 1
    # ResNetV2:
    #   num_layers: 50
    #   width_factor: 1

  
